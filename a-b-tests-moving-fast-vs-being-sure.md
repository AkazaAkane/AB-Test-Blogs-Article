---
description: Marton Trencseni - Mon 01 July 2019
---

# A/B tests: Moving Fast vs Being Sure

## My Thoughts

The author talks about a trade-off between speed and confidence: high alpha requires less sample size, but decreases confidence level. But in some situation it is worthwhile.

### Frequentist Workflow

* formulate a hypothesis ("sending additional notifications will cause people to be available for deliveries")
* select a target metric ("Delivery Performance = Deliveries/Dispatches") and specify the base value ("75%")
* estimate the lift on the target metric ("1%")
* use an off-the-shelf A/B testing tool to figure out how many N samples you will need, given the base metric value and expected lift

### Sample Size Determinants

* **α:** also called **sensitivity**, the probability that, if the experiment goes off, the effect is actually not there (**False Positive Rate, FPR**)
* **1-β:** also called **power**, the probability that, if the effect is there, the experiment will go off (**True Positive Rate, TPR**)

### Tradeoff

More power will enable us to catch more working experiments,but we will need to collect more N samples for this.

AB test is used for speed things up, but in some setting it is slow to collect N samples.

### Example: Toy Startup

* base metric = 75%, expected lift = +1%.
* Let's look at three different scenarios, α=0.05, α=0.10 and α=0.20.
* Let's assume that 1 in 4 experiments actually yield a hit, which results in the desired +1% lift.
* Let's estimate the value of a hit at 100,000 (we realize this on true positives, TPs),
* Let's estimate the cost of a rollout at $25,000 (we incur this cost on both TP and FP hits).

In this base scenario, with these parameters, running at high α is better. It yields more experiments, because we need less sample size, so we will find more hits, and even though we incur more false positives, it's still worth it. As we increase the cost of the rollout, this benefit goes away, as the cost of rolling out the FPs eats up the value generated by the TPs. **Having said that, I think this scenario applies in many startups: moving fast over being sure is worth it**.

### Conclusion

It does make sense to increase α, the accepted false positive rate, in some situations. For example, at my current company the base case is the closest to reality, except the value of a hit is even higher and the rollout cost is even lower (and the likelihood of a loss on a FP is low, sending more notifications is unlikely to yield less likely availability for delivery when our driver shows up).

But higher α leads to other issues:

* less certainty (higher FPR) could diminish the organizational belief in experimental results and A/B testing
* can a startup come up with enough experiments to justify a higher α?
* can a startup roll out all those (TP and FP) hits, is the velocity of the Product and Engineering team high enough?

### Reference

{% embed url="https://bytepawn.com/ab-tests-moving-fast-vs-being-sure.html#ab-tests-moving-fast-vs-being-sure" %}
