# Diluted Treatment Effect Estimation for Trigger Analysis

Many online services, especially a complex technology such as search, has many features that only affects a small proportion of traffic.&#x20;

For example, a better search result for weather prediction only triggers when user search some- thing related to weather. Some low coverage features trigger for less than 0.1% traffics. For a low coverage feature, straightforwardly perform hy- pothesis testing on all up data can suffer from low statistical power.&#x20;

For a feature with coverage less than 20%, we require team to also provide trigger analysis. Definition of triggering at each impression level (e.g.page- view or query-view) is conceptually easy. We know ex- actly whether a feature triggered or not in treatment, and with proper counterfactual logging, we should be able to tell whether a feature would be triggered if it were in treatment for control group. However, many metrics are only defined at experiment unit level, typically user in online A/B testing. To define many metrics properly, we cannot simply filter out triggered impressions for trigger analysis. To see that, a session is defined as a set of consecutive impressions from a user and we define a successful session if we believe user completed his or her “task” successfully in the session. And if we filter down to triggered impressions, we might only get a subset of impressions for each sessions of a user. Without the holistic view of the whole session we cannot properly define session success.&#x20;

Another example is that sometimes a feature is triggered to help user complete their task easier in follow on impressions within the same session. For instance, speller correction suggestion is triggered when we detect a potential misspelling. A user can click the suggestion and get better results if that is what they really want to query and achieve task completion on that next corrected query。

Due to the reason aforementioned, in search engine evalu- ation and many other web services, we need to keep session intact when doing trigger analysis. This leads to the concept of session-trigger analysis, in which we take the subset of ses- sions with at least one triggering event. In particular, users who never trigger the feature will be excluded from the anal- ysis. See Figure 2 for an illustration. However, some feature might have long lasting effects that will not only affect the current session but also affect subsequent sessions. In this case, similar to the logic that we want to maintain the whole session structure when doing trigger analysis, we might also want to keep all sessions after the first triggering event of a user. We can remove sessions before the first triggering event for obvious reason that a feature cannot impact any- thing before the first exposure. This leads us to the second type of trigger analysis. We call it user-trigger analysis. In Figure 2, we illustrate the difference between all up analysis, session-trigger and user-trigger analysis. The first trigger event is in session 2, so session 1 will be excluded from the analysis. Session 3 is included in user-trigger analysis but not in the session-trigger analysis.

The advantage of using session-trigger is more sensitive by further zooming into the feature im- pacted subset. Session-trigger analysis supports all session based metrics such as session success rate per user, session time to success per user, as well as all page based metrics such as click through rate, conversion rate, and other ratio metrics.&#x20;

It is important to keep in mind that for session- trigger analysis we assume the treatment has no impact on sessions without triggering. This is also an assumption that we can verify by doing trigger-complement analysis, in which we analyze the complement data of session-trigger analysis and test whether there is indeed no evidence of any impact.

[https://alexdeng.github.io/public/files/wsdm2015-dilution.pdf](https://alexdeng.github.io/public/files/wsdm2015-dilution.pdf)
